{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import os\n",
    "import sys\n",
    "import logging # for logging\n",
    "import shutil #to delete the directory contents\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "import logging.handlers\n",
    "import time\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "from numbers import Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-19_22_56_54.log\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "logger=logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logfile1 = time.strftime(\"%Y-%m-%d_%H_%M_%S\"+\".log\")\n",
    "print (logfile1)\n",
    "handler= logging.FileHandler(logfile1)\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter= logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Scraping the website and storing the content in four seperate lists')  \n",
    "# Use requests to get the contents\n",
    "url = \"http://www.freddiemac.com/pmms/pmms_archives.html\"\n",
    "r = requests.get(url)                                               \n",
    "\n",
    "# Get the text of the contents\n",
    "html_content = r.text\n",
    "\n",
    "# Convert the html content into a beautiful soup object\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "from datetime import datetime\n",
    "\n",
    "# r = requests.get(URL)\n",
    "link=[]\n",
    "texts=[]\n",
    "response=[]\n",
    "date=[]\n",
    "for links in soup.findAll('a', attrs={'href': re.compile(\"^http\")}):\n",
    "    texts.append(links.text)\n",
    "    a=(links.attrs['href'])\n",
    "    link.append(a)        \n",
    "    request = requests.get(a)\n",
    "    if request.status_code == 200:\n",
    "        response.append('1')\n",
    "    else:\n",
    "        response.append('0')\n",
    "    date.append(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Creating a Data frame and writing it as csv')\n",
    "df = pd.DataFrame(\n",
    "    {'Links': link,\n",
    "     'Description': texts,\n",
    "     'Response': response,\n",
    "     'date':date\n",
    "    })\n",
    "import time\n",
    "Date= time.strftime('%Y-%m-%d')\n",
    "file='Result'+'_'+Date+'.csv'\n",
    "df.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
